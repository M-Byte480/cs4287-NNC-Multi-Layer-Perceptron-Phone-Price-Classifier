{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:10:14.159514Z",
     "start_time": "2024-10-07T20:10:14.068838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# Milan Kovacs - 21308128, Caoimhe Cahill - 21331308\n",
    "# The code runs to execution\n",
    "\n",
    "\"\"\"\n",
    "Work to be done:\n",
    "    - Loading of data    # Caoimhe\n",
    "    - Processing of data # Caoimhe\n",
    "    - Define optimiser   # Milan 1 done\n",
    "    - Define loss function # Caoimhe\n",
    "    - Define activation function (sigmoid vs others) # Milan done\n",
    "    - Train\n",
    "        - Save # Milan\n",
    "        - Load # Milan\n",
    "    - Add comments explaining code\n",
    "    - Playing with Hyperparameters and noting the results\n",
    "        - Matplotlib \n",
    "            - Training graph\n",
    "            - Accuracy\n",
    "    - Evaluation \n",
    "\"\"\""
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWork to be done:\\n    - Loading of data    # Caoimhe\\n    - Processing of data # Caoimhe\\n    - Define optimiser   # Milan 1 done\\n    - Define loss function # Caoimhe\\n    - Define activation function (sigmoid vs others) # Milan done\\n    - Train\\n        - Save # Milan\\n        - Load # Milan\\n    - Add comments explaining code\\n    - Playing with Hyperparameters and noting the results\\n        - Matplotlib \\n            - Training graph\\n            - Accuracy\\n    - Evaluation \\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:10:14.252024Z",
     "start_time": "2024-10-07T20:10:14.235919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "from typing import Callable, List\n",
    "import numpy as np\n"
   ],
   "id": "d9516f3f7107b233",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:10:14.558561Z",
     "start_time": "2024-10-07T20:10:14.426143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constructor\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, \n",
    "                 layers: List[int], \n",
    "                 provided_loss_function: Callable[[float, float], float] = print, # todo: decide whether we pass these functions or call a switch case\n",
    "                 provided_optimiser: Callable[['MultiLayerPerceptron'], float] = print,\n",
    "                 provided_activation_function: Callable[[float], float] = print,\n",
    "                 provided_activation_function_prime: Callable[[float], float] = print,\n",
    "                 training_data: List[any] = None,\n",
    "                 test_data: List[any] = None,                  \n",
    "                 learning_rate: float = 0.2, \n",
    "                 epoch: int = 1_000,\n",
    "                 mini_batch_size: int = 100,\n",
    "                 k_fold: int = 6, \n",
    "                 split_data: bool = False) -> None:\n",
    "        # Declarations\n",
    "        if test_data is None:\n",
    "            test_data = []\n",
    "            \n",
    "        if training_data is None:\n",
    "            training_data = []\n",
    "        \n",
    "        self.layers  = layers\n",
    "        self.num_layers = len(self.layers)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epoch = epoch\n",
    "        self.training_data = training_data\n",
    "        self.test_data  = test_data\n",
    "        self.k_fold = k_fold\n",
    "        self.config_data = split_data\n",
    "        self.loss_function = provided_loss_function\n",
    "        self.activation_function = provided_activation_function\n",
    "        self.activation_function_prime = provided_activation_function_prime\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.optimiser = provided_optimiser\n",
    "        self.weights = []\n",
    "        self.biases  = []\n",
    "        print(str(self))\n",
    "        # Setting up\n",
    "        self.set_weights()\n",
    "        self.set_biases()\n",
    "        self.optimiser(self)\n",
    "\n",
    "\n",
    "        # Transform data\n",
    "        \n",
    "        # Completion\n",
    " \n",
    "    \n",
    "    \"\"\"\n",
    "        * Generates a list of array shapes for each weight that corresponds to each weight in a given layer.\n",
    "        * for example: if our layer is [4, 5, 2, 1], then we will have two lists to zip through\n",
    "        * [4, 5, 2] and [5, 2, 1] which means on first iteration, we have our input layer of [4] nodes connecting to each [5] nodes \n",
    "        * in the first hidden layer. This will generate a shape of 5 lists, with 4 weights each.\n",
    "        * Each shape corresponds to a node within our hidden layer (of 5 nodes) and in each shape the index \n",
    "        * corresponds to the index of our input layer. This is applied for the rest of the layers. \n",
    "    \"\"\"\n",
    "    def set_weights(self) -> None:\n",
    "        layers_after_input_layer = self.layers[1:]\n",
    "        layers_before_output_layer = self.layers[:-1]\n",
    "        self.weights = [\n",
    "            np.random.randn(y, x)\n",
    "            for x, y in zip(layers_before_output_layer, layers_after_input_layer)\n",
    "        ]\n",
    "    \n",
    "    \"\"\"\n",
    "        * Generates a list of Y by 1 dimensional list, where Y is the given layer size.\n",
    "        * This means we can correspond each index to a layer of biases,\n",
    "        * and within it, we can correspond each bias to a given node inside of that layer\n",
    "    \"\"\"\n",
    "    def set_biases(self) -> None:\n",
    "        layers_after_input = self.layers[1:]\n",
    "        self.biases = [\n",
    "            np.random.randn(y, 1) \n",
    "            for y in layers_after_input\n",
    "        ]\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch: List[tuple[any, any]]) -> None:\n",
    "        lr = self.learning_rate\n",
    "        nabla_biases = [np.zeros(b.shape) for b in self.biases]   # Derivative of biases\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights] # Derivative of weights\n",
    "        \n",
    "        \"\"\"\n",
    "            * We will iterate through the inputs and expected outputs of our mini batch provided\n",
    "            * \n",
    "        \"\"\"\n",
    "        for _input, expected_output in mini_batch:\n",
    "            delta_nabla_biases, delta_nabla_weights = self.backprop(_input, expected_output)\n",
    "            nabla_biases = [nabla_bias + delta_nabla_bias for nabla_bias, delta_nabla_bias in zip(nabla_biases, delta_nabla_biases)]\n",
    "            nabla_weights = [nabla_weight + delta_nabla_weight for nabla_weight, delta_nabla_weight in zip(nabla_weights, delta_nabla_weights)]\n",
    "            \n",
    "        \"\"\"\n",
    "           * [weight - (lr / len(mini_batch)) * nabla_weight\n",
    "                        for weight, nabla_weight in zip(self.weights, nabla_weights)]\n",
    "           * This is the delta rule applied to all the weights and biases. \n",
    "           *                new_weight = weight - derivative_weight * learning_rate\n",
    "           *\n",
    "           * We have our weights and the delta_weights (nabla_weights) and we iterate through the two in parallel\n",
    "           * We divide the learning_rate by the number of items in our mini_batch to keep it proportional\n",
    "           * \n",
    "           * This same concept is applied to the biases\n",
    "        \"\"\"\n",
    "        self.weights = [weight - (lr / len(mini_batch)) * nabla_weight\n",
    "                        for weight, nabla_weight in zip(self.weights, nabla_weights)]\n",
    "        self.biases = [bias - (lr / len(mini_batch)) * nabla_bias\n",
    "                       for bias, nabla_bias in zip(self.biases, nabla_biases)]\n",
    "    \n",
    "    \"\"\"\n",
    "        * \n",
    "    \"\"\"\n",
    "    def backprop(self, _in, _expected) -> (List, List):\n",
    "        nabla_biases =  [np.zeros(b.shape) for b in self.biases]   # Derivative of biases\n",
    "        nabla_weights = [np.zeros(w.shape) for w in self.weights] # Derivative of weights\n",
    "        \n",
    "        activation  = _in    # The inputted activation\n",
    "        activations = [_in]  # We will store all the activations, so we know which nodes are triggered\n",
    "        outputs     = []   \n",
    "        \n",
    "        \n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            output = np.dot(weight, activation) + bias # What is calculated by the node\n",
    "            outputs.append(output)         # Store all the outputs for the layer\n",
    "            activation = self.activation_function(output)   # Activation for current node\n",
    "            activations.append(activation) # Activations for the layer provided\n",
    "            \n",
    "        # Backward pass\n",
    "        delta = self.loss_function(activations[-1], _expected) * self.activation_function_prime(outputs[-1])\n",
    "        nabla_biases[-1] = delta\n",
    "        nabla_weights[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for layer in range(2, self.num_layers):\n",
    "            output = outputs[-layer]\n",
    "            _optimiser_prime = self.activation_function_prime(output)\n",
    "            delta = np.dot(self.weights[-layer+1].transpose(), delta) * _optimiser_prime\n",
    "            nabla_biases[-layer] = delta\n",
    "            nabla_weights[-layer] = np.dot(delta, activations[-layer-1].transpose())\n",
    "        \n",
    "        return nabla_biases, nabla_weights\n",
    "    \n",
    "    def evaluate(self, data):\n",
    "        test_results = [\n",
    "            (np.argmax(self.feedforward(_in)), expected)\n",
    "            for (_in, expected) in data\n",
    "        ]\n",
    "        return sum(int(_in == expected) for (_in, expected) in test_results)\n",
    "\n",
    "    def feedforward(self, _in):\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            _in = self.activation_function(np.dot(weight, _in) + bias)\n",
    "        return _in\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        string = \"Network generated with the following layers: \" + str(self.layers) +       \"\\n\" + \\\n",
    "        \"num_layers = \"+     str(self.num_layers) +                                         \"\\n\" + \\\n",
    "        \"learning_rate = \"+    str(self.learning_rate) +                                    \"\\n\" + \\\n",
    "        \"epoch = \"+            str(self.epoch) +                                            \"\\n\" + \\\n",
    "        \"training_data = \"+    str(len(self.training_data)) +                               \"\\n\" + \\\n",
    "        \"test_data = \"+        str(len(self.test_data)) +                                   \"\\n\" + \\\n",
    "        \"k_fold = \"+           str(self.k_fold) +                                           \"\\n\" + \\\n",
    "        \"config_data = \"+      str(self.config_data) +                                      \"\\n\" + \\\n",
    "        \"loss_function = \"+    str(self.loss_function.__name__) +                           \"\\n\" + \\\n",
    "        \"activation_function = \"+ str(self.activation_function.__name__) +                  \"\\n\" + \\\n",
    "        \"activation_function_prime = \"+ str(self.activation_function_prime.__name__) +      \"\\n\" + \\\n",
    "        \"mini_batch_size = \"+ str(self.mini_batch_size) +                                   \"\\n\" + \\\n",
    "        \"optimiser = \"+ str(self.optimiser.__name__)\n",
    "        return string\n"
   ],
   "id": "923833d8c4598184",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:10:14.619579Z",
     "start_time": "2024-10-07T20:10:14.600935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimisers\n",
    "\"\"\"\n",
    "    * Rewritten from - http://neuralnetworksanddeeplearning.com/chap1.html \n",
    "    * by Michael Nielsen 2019\n",
    "    \n",
    "    * It takes in an instance of our MLP\n",
    "    * The function extracts the properties from our MLP such as\n",
    "    * test_data, epochs, mini_batch_sizes, training_data\n",
    "\"\"\"\n",
    "def stochastic_gradient_descent(_mlp: 'MultiLayerPerceptron'):\n",
    "    _test_data: list = _mlp.test_data\n",
    "    epochs: int = _mlp.epoch\n",
    "    _mini_batch_size: int = _mlp.mini_batch_size\n",
    "    _training_data: list = _mlp.training_data\n",
    "    \n",
    "    if _test_data is not None:\n",
    "        n_test = len(_test_data) # If test_date is provided, then we will grab the number of iterations run for tests\n",
    "        \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(_training_data)\n",
    "        \n",
    "        mini_batches = populate_mini_batch(_training_data, _mini_batch_size)\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            _mlp.update_mini_batch(mini_batch)\n",
    "        \n",
    "        if _test_data is not None:\n",
    "            correctly_predicted_training_data = _mlp.evaluate(_test_data)\n",
    "            print(f\"Epoch {epoch}: {correctly_predicted_training_data} / {n_test}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch} complete\")\n",
    "\n",
    "def Adam(mlp: 'MultiLayerPerceptron'):\n",
    "    pass\n",
    "\n",
    "def cross_entropy(mlp: 'MultiLayerPerceptron'):\n",
    "    pass\n",
    "\n",
    "# Helper\n",
    "\"\"\"\n",
    "    * The function takes in training_data which is a list of the training data, and the batch sizes is the size of chunks \n",
    "    * we want to split our data set into. The `for iteration in range(0, n_training_data, batch_size)` iterates from 0 to \n",
    "    * the number of elements in the training data. Each iteration increments by the batch size number. Then on each \n",
    "    * iteration we get a range of elements from the current iteration -> iteration + batch size. This creates evenly\n",
    "    * distributed chunks.\n",
    "\"\"\"\n",
    "def populate_mini_batch(_training_data, batch_size) -> list:\n",
    "    n_training_data = len(_training_data)\n",
    "\n",
    "    return [\n",
    "            _training_data[iteration:iteration + batch_size]\n",
    "            for iteration in range(0, n_training_data, batch_size)\n",
    "        ]"
   ],
   "id": "72cc68d6dd3e88e7",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:10:14.681782Z",
     "start_time": "2024-10-07T20:10:14.669187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Activation functions\n",
    "\n",
    "# http://neuralnetworksanddeeplearning.com/chap1.html \n",
    "# ~ Michael Nielsen 2019\n",
    "threshold = 0\n",
    "\n",
    "def sigmoid(y: float):\n",
    "    return 1.0 / (1.0 + np.exp(-y))\n",
    "\n",
    "def sigmoid_prime(y: float):\n",
    "    return sigmoid(y) * (1 - sigmoid(y))\n",
    "    \n",
    "def stepper(y: float):\n",
    "    return 1 if y >= threshold else 0\n",
    "\n",
    "def stepper_prime(y: float):\n",
    "    return 0\n",
    "\n",
    "def tanh(y: float):\n",
    "    return np.tanh(y)\n",
    "\n",
    "def tanh_prime(y: float):\n",
    "    pass\n",
    "\n",
    "def rectified_linear_unit(y: float):\n",
    "    pass\n",
    "\n",
    "def rectified_linear_unit_prime(y: float):\n",
    "    pass"
   ],
   "id": "946294b609e9d7b6",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:10:14.738560Z",
     "start_time": "2024-10-07T20:10:14.729426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loss Functions\n",
    "def mean(output_activations: float, actual: float) -> float:\n",
    "    \"\"\"Return the vector of partial derivatives partial C_x partial a for the output activations.\"\"\"\n",
    "    return output_activations-actual\n",
    "\n",
    "def mean_squared_error(output_activations, actual):\n",
    "    return np.square(output_activations - actual)"
   ],
   "id": "453982d27daa459b",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T20:11:04.740865Z",
     "start_time": "2024-10-07T20:10:14.785754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# K-Fold for training and testing data\n",
    "import mnist_loader\n",
    "learning_rate = 3.0\n",
    "epoch = 30     \n",
    "mini_batch_size = 10\n",
    "\n",
    "training_data, validation_data, test_data1 = mnist_loader.load_data_wrapper()\n",
    "mlp = MultiLayerPerceptron(\n",
    "    layers = [784, 30, 10], \n",
    "    provided_loss_function = mean,\n",
    "    provided_optimiser = stochastic_gradient_descent,\n",
    "    provided_activation_function = sigmoid, \n",
    "    provided_activation_function_prime = sigmoid_prime,\n",
    "    training_data = training_data,\n",
    "    test_data = test_data1,\n",
    "    learning_rate = learning_rate,\n",
    "    epoch = epoch\n",
    ")\n",
    "\n"
   ],
   "id": "9b906e762b561b24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network generated with the following layers: [784, 30, 10]\n",
      "num_layers = 3\n",
      "learning_rate = 3.0\n",
      "epoch = 30\n",
      "training_data = 50000\n",
      "test_data = 10000\n",
      "k_fold = 6\n",
      "config_data = False\n",
      "loss_function = mean\n",
      "activation_function = sigmoid\n",
      "activation_function_prime = sigmoid_prime\n",
      "mini_batch_size = 100\n",
      "optimiser = stochastic_gradient_descent\n",
      "Epoch 0: 7220 / 10000\n",
      "Epoch 1: 8377 / 10000\n",
      "Epoch 2: 8668 / 10000\n",
      "Epoch 3: 8857 / 10000\n",
      "Epoch 4: 8941 / 10000\n",
      "Epoch 5: 8996 / 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[129], line 8\u001B[0m\n\u001B[0;32m      5\u001B[0m mini_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m\n\u001B[0;32m      7\u001B[0m training_data, validation_data, test_data1 \u001B[38;5;241m=\u001B[39m mnist_loader\u001B[38;5;241m.\u001B[39mload_data_wrapper()\n\u001B[1;32m----> 8\u001B[0m mlp \u001B[38;5;241m=\u001B[39m MultiLayerPerceptron(\n\u001B[0;32m      9\u001B[0m     layers \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m784\u001B[39m, \u001B[38;5;241m30\u001B[39m, \u001B[38;5;241m10\u001B[39m], \n\u001B[0;32m     10\u001B[0m     provided_loss_function \u001B[38;5;241m=\u001B[39m mean,\n\u001B[0;32m     11\u001B[0m     provided_optimiser \u001B[38;5;241m=\u001B[39m stochastic_gradient_descent,\n\u001B[0;32m     12\u001B[0m     provided_activation_function \u001B[38;5;241m=\u001B[39m sigmoid, \n\u001B[0;32m     13\u001B[0m     provided_activation_function_prime \u001B[38;5;241m=\u001B[39m sigmoid_prime,\n\u001B[0;32m     14\u001B[0m     training_data \u001B[38;5;241m=\u001B[39m training_data,\n\u001B[0;32m     15\u001B[0m     test_data \u001B[38;5;241m=\u001B[39m test_data1,\n\u001B[0;32m     16\u001B[0m     learning_rate \u001B[38;5;241m=\u001B[39m learning_rate,\n\u001B[0;32m     17\u001B[0m     epoch \u001B[38;5;241m=\u001B[39m epoch\n\u001B[0;32m     18\u001B[0m )\n",
      "Cell \u001B[1;32mIn[125], line 42\u001B[0m, in \u001B[0;36mMultiLayerPerceptron.__init__\u001B[1;34m(self, layers, provided_loss_function, provided_optimiser, provided_activation_function, provided_activation_function_prime, training_data, test_data, learning_rate, epoch, mini_batch_size, k_fold, split_data)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_weights()\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_biases()\n\u001B[1;32m---> 42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimiser(\u001B[38;5;28mself\u001B[39m)\n",
      "Cell \u001B[1;32mIn[126], line 29\u001B[0m, in \u001B[0;36mstochastic_gradient_descent\u001B[1;34m(_mlp)\u001B[0m\n\u001B[0;32m     26\u001B[0m     _mlp\u001B[38;5;241m.\u001B[39mupdate_mini_batch(mini_batch)\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _test_data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 29\u001B[0m     correctly_predicted_training_data \u001B[38;5;241m=\u001B[39m _mlp\u001B[38;5;241m.\u001B[39mevaluate(_test_data)\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcorrectly_predicted_training_data\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m / \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_test\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[1;32mIn[125], line 142\u001B[0m, in \u001B[0;36mMultiLayerPerceptron.evaluate\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, data):\n\u001B[0;32m    141\u001B[0m     test_results \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 142\u001B[0m         (np\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeedforward(_in)), expected)\n\u001B[0;32m    143\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m (_in, expected) \u001B[38;5;129;01min\u001B[39;00m data\n\u001B[0;32m    144\u001B[0m     ]\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mint\u001B[39m(_in \u001B[38;5;241m==\u001B[39m expected) \u001B[38;5;28;01mfor\u001B[39;00m (_in, expected) \u001B[38;5;129;01min\u001B[39;00m test_results)\n",
      "Cell \u001B[1;32mIn[125], line 149\u001B[0m, in \u001B[0;36mMultiLayerPerceptron.feedforward\u001B[1;34m(self, _in)\u001B[0m\n\u001B[0;32m    147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeedforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, _in):\n\u001B[0;32m    148\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m bias, weight \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbiases, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights):\n\u001B[1;32m--> 149\u001B[0m         _in \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_function(np\u001B[38;5;241m.\u001B[39mdot(weight, _in) \u001B[38;5;241m+\u001B[39m bias)\n\u001B[0;32m    150\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _in\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 129
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
